resume_from_checkpoint_path: null # only used for resume_from_checkpoint option in PL
result_path: "/mnt/petrelfs/donut/result/donut_base/"
pretrained_model_name_or_path: "/mnt/petrelfs/donut/result/donut_base/binmodel/epoch9" # loading a pre-trained model (from moldehub or path)
dataset_name_or_paths: ["/mnt/petrelfs/plotQA/val/annotations3.json"] # loading datasets (from moldehub or path)
sort_json_key: False # cord dataset is preprocessed, and publicly available at https://huggingface.co/datasets/naver-clova-ix/cord-v2
train_batch_sizes: [8]
val_batch_sizes: [1]
input_size: [1280, 960] # when the input resolution differs from the pre-training setting, some weights will be newly initialized (but the model training would be okay)
max_length: 1536
max_position_embeddings: 1536
encoder_layer: [2, 2, 14, 2]
decoder_layer: 4
window_size: 10
align_long_axis: False
num_nodes: 2
seed: 2022
lr: 5e-5
warmup_steps: 250 # 800/8*30/10, 10%
num_training_samples_per_epoch: 20000
max_epochs: 10
max_steps: -1
num_workers: 16
val_check_interval: 1.0
check_val_every_n_epoch: 10
gradient_clip_val: 1.0
verbose: True
